#!/usr/bin/env python3
"""Fetch merged OSS contributions and update the site data snippet."""

from __future__ import annotations

import argparse
import html
import json
import os
import sys
from pathlib import Path
from typing import Iterable
from urllib import error, parse, request

SEARCH_URL = "https://api.github.com/search/issues"
SEARCH_PAGE_SIZE = 50
REPO_ROOT = Path(__file__).resolve().parent.parent
DATA_TARGET = REPO_ROOT / "src" / "site" / "_data" / "contributions.html"


def fetch_contributions(login: str, limit: int, token: str | None) -> list[dict]:
    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "rafsaf.pl-contributions-fetcher",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    collected: list[dict] = []
    page = 1
    query = f"author:{login} type:pr is:merged"

    skip_prefixes = {f"{login}/", "rycerski/"}

    while len(collected) < limit:
        params = parse.urlencode(
            {
                "q": query,
                "sort": "updated",
                "order": "desc",
                "per_page": SEARCH_PAGE_SIZE,
                "page": page,
            }
        )
        url = f"{SEARCH_URL}?{params}"
        req = request.Request(url, headers=headers)
        with request.urlopen(req) as resp:  # type: ignore[call-arg]
            if resp.status != 200:  # pragma: no cover - urllib raises before this
                raise RuntimeError(f"GitHub API returned HTTP {resp.status}")
            payload = json.load(resp)

        items = payload.get("items", [])
        if not items:
            break

        for item in items:
            repo_url = item.get("repository_url", "")
            owner_repo = ""
            if repo_url.startswith("https://api.github.com/repos/"):
                owner_repo = repo_url.split("/repos/", 1)[-1]
            if any(owner_repo.startswith(prefix) for prefix in skip_prefixes):
                continue
            collected.append(item)
            if len(collected) >= limit:
                break

        if len(items) < SEARCH_PAGE_SIZE:
            break

        page += 1
        if page > 10:  # safety guard against looping endlessly
            break

    return collected


def guess_repo_name(repository_url: str) -> str:
    if "/repos/" not in repository_url:
        return repository_url.rsplit("/", 1)[-1]
    return repository_url.split("/repos/", 1)[-1]


def format_contribution(item: dict) -> str:
    pr_title = html.escape(item.get("title", "Untitled contribution").strip())
    pr_url = item.get("html_url", "")
    repo_name = html.escape(guess_repo_name(item.get("repository_url", "")))
    repo_url = f"https://github.com/{repo_name}" if repo_name else pr_url

    merged_at = (
        item.get("pull_request", {}).get("merged_at")
        or item.get("closed_at")
        or item.get("updated_at")
        or ""
    )
    merged_year = (merged_at or "0000")[:4]

    number = item.get("number")
    suffix = f" #{number}" if isinstance(number, int) else ""

    pr_anchor = (
        f'<a href="{pr_url}" target="_blank" rel="noreferrer">{pr_title}{suffix}</a>'
    )
    repo_anchor = (
        f'<span class="projects-contribution-repo">in '
        f'<a href="{repo_url}" target="_blank" rel="noreferrer">{repo_name}</a></span>'
        if repo_name
        else ""
    )

    return (
        '    <div class="projects-others-text projects-contribution">'
        f"({merged_year}) {pr_anchor}{repo_anchor}"
        "</div>"
    )


def render_html(items: Iterable[dict]) -> str:
    body = ["<!-- Auto-generated by scripts/update_contributions.py -->", ""]
    formatted = [format_contribution(item) for item in items]

    if not formatted:
        body.append(
            '<div class="projects-others-text">No merged pull requests found right now.'
            "</div>"
        )
    else:
        body.extend(formatted)

    body.append("")
    return "\n".join(body)


def main() -> int:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--login", default="rafsaf", help="GitHub login to inspect")
    parser.add_argument(
        "--limit",
        "--per-page",
        dest="limit",
        type=int,
        default=100,
        help="How many merged pull requests to list (will paginate as needed)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=DATA_TARGET,
        help="Where to write the generated HTML snippet",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print the generated HTML to stdout instead of writing it",
    )
    args = parser.parse_args()

    token = os.getenv("GITHUB_TOKEN") or os.getenv("GH_TOKEN")

    try:
        items = fetch_contributions(args.login, max(args.limit, 1), token)
    except error.HTTPError as exc:  # pragma: no cover - network errors
        message = exc.read().decode() if hasattr(exc, "read") else str(exc)
        print(f"GitHub API error: {exc.code}\n{message}", file=sys.stderr)
        return exc.code
    except error.URLError as exc:  # pragma: no cover - network errors
        print(f"Network error: {exc.reason}", file=sys.stderr)
        return 1

    html_payload = render_html(items)

    if args.dry_run:
        print(html_payload)
        return 0

    args.output.parent.mkdir(parents=True, exist_ok=True)
    args.output.write_text(html_payload)
    print(f"Wrote {len(items)} contributions to {args.output}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
