<h1>Cross-site Proxmox networking (Rozalin + Wro): WireGuard underlay + Proxmox SDN (EVPN/VXLAN)</h1>
<p>This is a technical write-up of how I connected two physically separate Proxmox clusters into one coherent private network.</p>
<p>The goal was simple:</p>
<ul>
<li>Put workloads in Rozalin and Wro on a single private L3 fabric.</li>
<li>Keep the underlay on the public Internet, but encrypted.</li>
<li>Make it manageable and reproducible (OpenTofu/Terraform + Ansible).</li>
</ul>
<p>The <em>reality</em> was mostly about understanding Proxmox SDN semantics, exit node behavior, Linux forwarding, and Proxmox firewall forwarding.</p>
<hr>
<h2>TL;DR (final working shape)</h2>
<ul>
<li><strong>Underlay:</strong> WireGuard between the Proxmox hosts.</li>
<li><strong>Overlay:</strong> Proxmox SDN <strong>EVPN zone</strong> controlled by <code>evpnctl</code> (FRR). VXLAN encapsulation runs over the WireGuard underlay.</li>
<li><strong>Per-site SDN subnets:</strong></li>
<li>Rozalin: <code>10.1.1.0/24</code> (gateway <code>10.1.1.1</code>)</li>
<li>Wro: <code>10.2.1.0/24</code> (gateway <code>10.2.1.1</code>)</li>
<li><strong>Egress:</strong> SDN subnet has <code>snat=true</code> (per site). Traffic from SDN to “outside” egresses via the site’s exit node.</li>
<li><strong>Critical fix:</strong> <code>exit_nodes_local_routing=false</code> (enabling “local routing” was the trap).</li>
<li><strong>Critical host sysctls:</strong> enable forwarding, disable rp_filter.</li>
<li><strong>Firewall:</strong> Proxmox cluster firewall must explicitly allow forwarding for SDN objects, not just “open the ports”.</li>
</ul>
<hr>
<h2>Architecture</h2>
<h3>Layering</h3>
<p>Think in two layers:</p>
<p>1) <strong>Underlay (encrypted L3)</strong>: WireGuard
2) <strong>Overlay (virtual L2/L3 fabric)</strong>: EVPN/VXLAN (Proxmox SDN)</p>
<pre class="codehilite"><code>             Public Internet
        (dynamic IPs, NAT, etc.)

   roz.rafsaf.pl                       wro.rafsaf.pl
   +----------------+                  +----------------+
   | Proxmox (Roz)  |                  | Proxmox (Wro)  |
   | WG: 10.1.0.1   |&lt;==== WireGuard ===&gt;| WG: 10.2.0.1   |
   | FRR + evpnctl  |  UDP/51820       | FRR + evpnctl  |
   +--------+-------+                  +--------+-------+
            |                                   |
            |             EVPN/VXLAN            |
            +====(BGP: TCP/179, VXLAN: UDP/4789)+
                           over WG
</code></pre>

<h3>Why WireGuard underlay instead of “SDN peers over DDNS”</h3>
<p>Early on I tried to make “cross-site VXLAN peers” work with DDNS endpoints. That is fragile:</p>
<ul>
<li>SDN VXLAN/EVPN needs stable peer addressing.</li>
<li>DDNS changes don’t map nicely to how Proxmox stores/uses peer endpoints.</li>
<li>Even if DNS updates correctly, you now debug: DNS → firewall → NAT → VXLAN.</li>
</ul>
<p>WireGuard gives you stable “site-to-site” tunnel IPs (<code>10.1.0.1</code>, <code>10.2.0.1</code>). SDN peers can then be defined in terms of those stable addresses.</p>
<hr>
<h2>Repository mapping (where the truth lives)</h2>
<h3>Underlay: Ansible (WireGuard + sysctls)</h3>
<ul>
<li>WireGuard is managed via Ansible role <code>githubixx.ansible_role_wireguard</code>.</li>
<li>Host variables (tunnel IPs, endpoints, peer AllowedIPs) are in <code>ansible/inventory/host_vars/*</code>.</li>
<li>Host prerequisites (FRR packages, evpnctl controller creation, sysctls) are in <code>ansible/roles/proxmox_host/tasks/main.yml</code>.</li>
</ul>
<p>Key sysctls:</p>
<ul>
<li><code>net.ipv4.ip_forward=1</code></li>
<li><code>net.bridge.bridge-nf-call-iptables=1</code></li>
<li><code>net.bridge.bridge-nf-call-ip6tables=1</code></li>
<li><code>net.ipv4.conf.all.rp_filter=0</code></li>
<li><code>net.ipv4.conf.default.rp_filter=0</code></li>
</ul>
<p>If you skip rp_filter here, you will burn hours “debugging EVPN” while the kernel quietly drops your asymmetric flows.</p>
<h3>Overlay: OpenTofu/Terraform (Proxmox SDN EVPN)</h3>
<p>Per cluster:</p>
<ul>
<li><code>proxmox_rozalin_server_bpg/sdn_evpn.tf</code></li>
<li><code>proxmox_wro_server_bpg/sdn_evpn.tf</code></li>
</ul>
<p>This defines:</p>
<ul>
<li>SDN EVPN zone</li>
<li>SDN VNet</li>
<li>SDN subnet</li>
<li>Exit node behavior</li>
</ul>
<p>A previous attempt used SDN VXLAN zone resources (see milestone commit below), but EVPN ended up being the path that worked and scaled.</p>
<h3>Firewall: OpenTofu/Terraform (Proxmox cluster firewall)</h3>
<p>Per cluster:</p>
<ul>
<li><code>proxmox_rozalin_server_bpg/cluster_firewall.tf</code></li>
<li><code>proxmox_wro_server_bpg/cluster_firewall.tf</code></li>
</ul>
<p>This is where most “it should work” gets murdered.</p>
<p>Two separate requirements:</p>
<p>1) <strong>Ports:</strong> allow WireGuard UDP 51820, BGP TCP 179, VXLAN UDP 4789.
2) <strong>Forwarding policy:</strong> allow forwarding between SDN objects and the outside.</p>
<hr>
<h2>SDN testing VM (sdntest): the NIC layout that finally stopped lying</h2>
<p>To validate SDN behavior without dragging real workloads into the blast radius I created a tiny VM in each cluster.</p>
<p>Final shape is <strong>two NICs</strong>:</p>
<ul>
<li><code>eth0</code>: attached to the SDN VNet (this becomes the overlay dataplane)</li>
<li><code>eth1</code>: attached to the “local mgmt” bridge (<code>vmbr0</code>)</li>
</ul>
<p>This matters because if you try to do everything on one NIC you conflate:</p>
<ul>
<li>“Can I reach the other site”</li>
<li>“Can the exit node NAT/forward”</li>
<li>“Can Proxmox firewall forward SDN traffic”</li>
<li>“Is the VM routing correctly”</li>
</ul>
<p>Two NICs makes it obvious which plane broke.</p>
<p>ASCII view:</p>
<pre class="codehilite"><code>     sdntest VM

      +--------------------+
      | eth0: vnet (SDN)   | 10.x.1.x/24
      |  - default gw =    | 10.x.1.1
      |    SDN gateway     |
      |                    |
      | eth1: vmbr0 (mgmt) | 192.168.x.x
      +--------------------+

 eth0 tests overlay + exit-node + SNAT
 eth1 keeps you in control when overlay is broken
</code></pre>

<p>Definitions live in:</p>
<ul>
<li><code>proxmox_rozalin_server_bpg/vm_sdntest.tf</code></li>
<li><code>proxmox_wro_server_bpg/vm_sdntest.tf</code></li>
</ul>
<hr>
<h2>Debugging story (the parts that actually mattered)</h2>
<h3>1) VXLAN zone attempt: it looked straightforward, it wasn’t</h3>
<p>Milestone commit:</p>
<ul>
<li><code>bc980a5 2026-01-10 sdn next step</code></li>
</ul>
<p>At this point, SDN VXLAN was defined with peers as WireGuard tunnel IPs and MTU 1370.</p>
<p>Conceptually fine. Practically, VXLAN didn’t give me the “control plane” visibility I needed. And the moment you bring dynamic endpoints (DDNS) into peer definition, you add another failure mode that Proxmox does not “helpfully explain”.</p>
<p>EVPN (BGP control plane) is more verbose and debuggable.</p>
<h3>2) EVPN introduced (and why it’s not just “turn it on”)</h3>
<p>Milestone commit:</p>
<ul>
<li><code>b0d5919 2026-01-12 sdn evpn</code></li>
</ul>
<p>EVPN isn’t magic. You now depend on:</p>
<ul>
<li>FRR being installed and working</li>
<li><code>evpnctl</code> controller created and configured</li>
<li>BGP reachability between controllers (TCP 179) on the underlay</li>
<li>VXLAN reachability between nodes (UDP 4789) on the underlay</li>
</ul>
<p>If any of those are blocked, your “L2 overlay” just silently doesn’t appear.</p>
<h3>3) The killer setting: <code>exit_nodes_local_routing</code></h3>
<p>This is the bug-shaped hole.</p>
<p>Proxmox EVPN zone supports “exit nodes” to provide a routed gateway / egress for SDN subnets (and to do SNAT if configured).</p>
<p>I initially assumed:</p>
<ul>
<li>“local routing” should be enabled because it sounds like “route locally, be efficient”.</li>
</ul>
<p>In practice for this design, <strong>enabling local routing broke cross-site behavior</strong>.</p>
<p>Final working config is:</p>
<ul>
<li><code>exit_nodes_local_routing = false</code></li>
</ul>
<p>Milestone commit:</p>
<ul>
<li><code>9674147 2026-01-12 exit_nodes</code></li>
</ul>
<p>If there is one takeaway from the whole saga, it is:</p>
<blockquote>
<p>Do not assume <code>exit_nodes_local_routing=true</code> is an optimization. Treat it as a behavioral change that can blackhole traffic.</p>
</blockquote>
<h3>4) The kernel wasn’t routing (because I didn’t tell it to)</h3>
<p>Milestone commit:</p>
<ul>
<li><code>eeab5cf 2026-01-14 bring back sysctl</code></li>
</ul>
<p>EVPN exit-node behavior is basically “you are a router now”. Linux defaults do not assume that.</p>
<p>Without the sysctls (forwarding + rp_filter disabled), you can have:</p>
<ul>
<li>WG working</li>
<li>BGP sessions up</li>
<li>VXLAN up</li>
<li>yet packets still die at the exit node.</li>
</ul>
<p>This is the worst category of problem: everything “looks up” but traffic doesn’t flow.</p>
<h3>5) Proxmox firewall forwarding: allow-list the SDN objects, not just raw ports</h3>
<p>Milestones:</p>
<ul>
<li><code>af89abc 2026-01-14 missing forward</code></li>
<li><code>4bc782a 2026-01-14 allow forwarding</code></li>
<li><code>09f9892 2026-01-14 accept</code></li>
</ul>
<p>I hit the classic Proxmox firewall trap:</p>
<ul>
<li>“I opened the underlay ports, why is SDN traffic still blocked?”</li>
</ul>
<p>Because SDN traffic isn’t just about underlay ports. Once the SDN vnet exists, you need to allow forwarding rules for SDN endpoints using Proxmox’s SDN object selectors (things like <code>+sdn/&lt;vnet&gt;-gateway</code> etc.).</p>
<p>If you don’t, the SDN gateway can exist and still be unable to forward.</p>
<h3>6) Conntrack / NAT symptoms that mislead you</h3>
<p>A lot of the time the broken behavior <em>looks like</em> conntrack weirdness:</p>
<ul>
<li>SYN goes out, SYN/ACK comes back, then it dies</li>
<li>or traffic works one way but not the other</li>
</ul>
<p>In this stack it’s usually one of:</p>
<ul>
<li>rp_filter dropping packets</li>
<li>forwarding off</li>
<li>Proxmox firewall forward rules missing</li>
<li>exit-node “local routing” doing something you didn’t intend</li>
</ul>
<p>Conntrack becomes the <em>visible</em> symptom, not the root cause.</p>
<hr>
<h2>Final configuration (what to verify when it’s broken)</h2>
<h3>Underlay checklist (WireGuard)</h3>
<ul>
<li><code>wg show</code> shows a recent handshake in both directions.</li>
<li><code>ping 10.1.0.1 &lt;-&gt; 10.2.0.1</code> works.</li>
<li>UDP 51820 open between sites.</li>
</ul>
<h3>EVPN control-plane checklist</h3>
<ul>
<li>FRR running.</li>
<li>BGP session established between <code>evpnctl</code> peers.</li>
<li>TCP 179 allowed over the WG underlay.</li>
</ul>
<h3>VXLAN dataplane checklist</h3>
<ul>
<li>UDP 4789 allowed over the WG underlay.</li>
<li>MTU reduced (1370) to avoid fragmentation through WG+VXLAN overhead.</li>
</ul>
<h3>Exit-node + NAT checklist</h3>
<ul>
<li><code>exit_nodes_local_routing=false</code>.</li>
<li>sysctls applied (<code>ip_forward=1</code>, rp_filter=0).</li>
<li>Proxmox firewall forwarding explicitly allows SDN gateway -&gt; 0.0.0.0/0.</li>
</ul>
<h3>Practical tests (sdntest)</h3>
<p>From Roz sdntest:</p>
<ul>
<li><code>ping 10.2.1.9</code> (Wro sdntest overlay IP)</li>
<li><code>curl</code> to something external to validate SNAT via exit node</li>
</ul>
<p>Then repeat from Wro.</p>
<hr>
<h2>Timeline of milestones (repo history)</h2>
<ul>
<li><code>c85e2dc 2026-01-09 wireguard</code></li>
<li><code>bc980a5 2026-01-10 sdn next step</code> (VXLAN SDN attempt)</li>
<li><code>550a2b0 2026-01-10 vm sdntest</code></li>
<li><code>b0d5919 2026-01-12 sdn evpn</code></li>
<li><code>9674147 2026-01-12 exit_nodes</code> (includes the key <code>exit_nodes_local_routing</code> behavior change)</li>
<li><code>eeab5cf 2026-01-14 bring back sysctl</code> (forwarding + rp_filter)</li>
<li><code>af89abc 2026-01-14 missing forward</code></li>
<li><code>4bc782a 2026-01-14 allow forwarding</code></li>
<li><code>09f9892 2026-01-14 accept</code></li>
</ul>
<hr>
<h2>Closing notes</h2>
<p>If you want to reproduce this setup elsewhere, stop thinking of Proxmox SDN as a “checkbox” feature.</p>
<p>Treat it like:</p>
<ul>
<li>a routing project (forwarding + rp_filter)</li>
<li>plus a firewall project (forward rules for SDN objects)</li>
<li>plus a control-plane project (BGP)</li>
</ul>
<p>And remember the one configuration bit that cost me the most time:</p>
<ul>
<li><code>exit_nodes_local_routing=false</code></li>
</ul>
<p>Last updated: 2025-02-05</p>
<p>Content license: CC BY-NC 4.0 — share and adapt with attribution, no commercial use.</p>